{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSUY6wbu1LA7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import mstats\n",
    "\n",
    "# Working with files\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "\n",
    "\n",
    "# Working with OpenML dataset\n",
    "import openml\n",
    "from openml.datasets import edit_dataset, fork_dataset, get_dataset\n",
    "\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelBinarizer, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "# Performance Metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureEnginering(df, train = False):\n",
    "    \n",
    "    if train == True:\n",
    "        \n",
    "        ##### Checking for each column the type and the number of distinct and similar values (also see if there are missing values)\n",
    "        missing_col = []\n",
    "\n",
    "        for col in df.columns:\n",
    "            if df[col].isna().sum() > 0:\n",
    "                missing_col.append(col)\n",
    "\n",
    "        ##### Filling missing values\n",
    "        for col in missing_col:\n",
    "            median_value = df[col].median()\n",
    "            df[col].fillna(median_value, inplace = True)\n",
    "\n",
    "\n",
    "        ##### Treating Outliers\n",
    "        values_to_exclude = ['Average_ACT_Score']\n",
    "        filtered_col = [x for x in missing_col if x not in values_to_exclude]\n",
    "\n",
    "        for col in missing_col:\n",
    "\n",
    "            # Get the minimum and maximum values in the column\n",
    "            min_value = df[col].min()\n",
    "            max_value = df[col].max()\n",
    "\n",
    "            # Adjust k based on the range of values\n",
    "            if max_value < 100:\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                if 100 <= max_value <= 700:\n",
    "                    k = 0.8\n",
    "                elif 700 <= max_value <= 5000:\n",
    "                    k = 1.7 \n",
    "                else:\n",
    "                    k = 2.5\n",
    "\n",
    "                # Calculate the interquartile range (IQR)\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "\n",
    "\n",
    "                # Lower and upper bounds\n",
    "                lower_bound = Q1 - k * IQR\n",
    "                upper_bound = Q3 + k * IQR\n",
    "\n",
    "                df[col] = df[col].clip(lower = lower_bound, upper = upper_bound)\n",
    "\n",
    "\n",
    "        ##### Mapping Categorical Features\n",
    "        states = list(set(df['State']))\n",
    "        mapped_states = {st: idx if st is not None else -1 for idx, st in enumerate(states)}\n",
    "        df['State'] = df['State'].replace(mapped_states)\n",
    "        \n",
    "        \n",
    "        target = {'N': 0, 'P': 1}\n",
    "        df['binaryClass'] = df['binaryClass'].map(target)\n",
    "\n",
    "\n",
    "        ##### Scaling\n",
    "#         scaler = StandardScaler()\n",
    "#         scaler.fit(X_train)\n",
    "\n",
    "#         X_train_scaled = scaler.transform(X_train)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype = torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype = torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_sizes, activation_funcs):\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        self.activation_funcs = activation_funcs\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_size)\n",
    "        # input layer\n",
    "        self.layers.append(nn.utils.weight_norm(nn.Linear(input_size, layer_sizes[0])))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(layer_sizes[0]))\n",
    "        self.dropouts.append(nn.Dropout(0.2))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            self.layers.append(nn.utils.weight_norm(nn.Linear(layer_sizes[i-1], layer_sizes[i])))\n",
    "            if i < len(layer_sizes) - 1:\n",
    "                self.batch_norms.append(nn.BatchNorm1d(layer_sizes[i]))\n",
    "            self.dropouts.append(nn.Dropout(0.5))\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.utils.weight_norm(nn.Linear(layer_sizes[-1], output_size))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropouts[0](x)\n",
    "        x = self.layers[0](x)\n",
    "        x = self.activation_funcs[0](x)\n",
    "\n",
    "        for i in range(1, len(self.layers)):\n",
    "            if i < len(self.batch_norms):\n",
    "                x = self.batch_norms[i](x)\n",
    "            x = self.dropouts[i](x)\n",
    "            x = self.layers[i](x)\n",
    "            if i < len(self.activation_funcs):\n",
    "                x = self.activation_funcs[i](x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(configs, dataset):\n",
    "\n",
    "    best_config = None\n",
    "    best_score = None\n",
    "    \n",
    "    scores = []\n",
    "#     best_scores = []\n",
    "    layers = []\n",
    "    activations = []\n",
    "\n",
    "    for config in configs:\n",
    "        best_score = 0\n",
    "        layer_size = config['layer_sizes']\n",
    "        activation = config['activations']\n",
    "\n",
    "        model = SimpleNN(\n",
    "            input_size = INPUT_SIZE, output_size = OUTPUT_SIZE,\n",
    "            layer_sizes = config['layer_sizes'],  activation_funcs = config['activations']\n",
    "        )\n",
    "\n",
    "        print(f\"Layer sizes: {layer_size}, Activations: {[type(act).__name__ for act in activation]}\")\n",
    "\n",
    "        score, loss, all_f1_scores = train_and_evaluate_model(model, dataset)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_config = config\n",
    "            \n",
    "        scores.append(all_f1_scores)\n",
    "#         best_scores.append(all_best_losses)\n",
    "        layers.append(layer_size)\n",
    "        activations.append([type(act).__name__ for act in activation])\n",
    "\n",
    "    print(f\"Best F1 Score: {best_score:.2f}, Layer sizes: {layer_size}, Activations: {[type(act).__name__ for act in activation]}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return best_config, best_score, scores, layers, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_search(base_config, dataset, iterations=10):\n",
    "    def get_neighbors(configuration):\n",
    "        neighbors = []\n",
    "\n",
    "        # Example: Add a layer\n",
    "        neighbors.append({\n",
    "            'layer_sizes': configuration['layer_sizes'] + [64],\n",
    "            'activations': configuration['activations'] + [nn.ReLU()]\n",
    "        })\n",
    "\n",
    "        # Example: Remove a layer\n",
    "        if len(configuration['layer_sizes']) > 1:\n",
    "            neighbors.append({\n",
    "                'layer_sizes': configuration['layer_sizes'][:-1],\n",
    "                'activations': configuration['activations'][:-1]\n",
    "            })\n",
    "        return neighbors\n",
    "\n",
    "    def get_model(config):\n",
    "        layer_size = config['layer_sizes']\n",
    "        activation = config['activations']\n",
    "        model = SimpleNN(INPUT_SIZE, OUTPUT_SIZE, layer_size, activation)\n",
    "\n",
    "        return model\n",
    "\n",
    "    current_configuration = base_config\n",
    "    model = get_model(current_configuration)\n",
    "    current_score, current_loss, _ = train_and_evaluate_model(model, dataset)\n",
    "\n",
    "    while True:\n",
    "        neighbors = get_neighbors(current_configuration)\n",
    "        any_improvement = False\n",
    "\n",
    "        for neighbor_config in neighbors:\n",
    "            neighbor_model = get_model(neighbor_config)\n",
    "            score, loss, _ = train_and_evaluate_model(neighbor_model, dataset)\n",
    "\n",
    "            if score > current_score:\n",
    "                current_configuration = neighbor_config\n",
    "                current_score = score\n",
    "                any_improvement = True\n",
    "                break  # Move to the first improving neighbor\n",
    "\n",
    "        if not any_improvement:\n",
    "            break  # No improvement found, stop the search\n",
    "\n",
    "    return current_configuration, current_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "\n",
    "    for data, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        final_loss += loss.item()\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    final_f1_score = 0\n",
    "\n",
    "    for data, target in dataloader:\n",
    "        outputs = model(data)\n",
    "        loss = loss_fn(outputs, target.long())\n",
    "        final_loss += loss.item()\n",
    "\n",
    "        preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
    "        final_f1_score += (f1_score(target.cpu().numpy(), preds.round(), average='weighted'))\n",
    "\n",
    "    final_f1_score /= len(dataloader)\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "    return final_loss, final_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and train a network\n",
    "def train_and_evaluate_model(model, dataset, n_splits = 5):\n",
    "\n",
    "\n",
    "    labels = [label for _, label in dataset]\n",
    "    kfold = StratifiedKFold(n_splits = n_splits, shuffle=True)\n",
    "    \n",
    "    all_f1_scores = []\n",
    "\n",
    "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(range(len(dataset)), labels)):\n",
    "\n",
    "        # Splitting the dataset\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "\n",
    "        # Creating data loaders\n",
    "        train_loader = DataLoader(dataset, batch_size = BATCH_SIZE, sampler = train_subsampler)\n",
    "        valid_loader = DataLoader(dataset, batch_size = BATCH_SIZE, sampler = valid_subsampler)\n",
    "\n",
    "        # initialization\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n",
    "                                  max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "        early_step = 0\n",
    "\n",
    "\n",
    "        # Training loop\n",
    "        best_loss = np.inf\n",
    "        for epoch in range(EPOCHS):\n",
    "\n",
    "            train_loss = train_fn(model, optimizer,scheduler, loss_fn, train_loader)\n",
    "            valid_loss, score = valid_fn(model, loss_fn, valid_loader)\n",
    "\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                f1_score = score\n",
    "\n",
    "                torch.save(model.state_dict(), f\"fold{fold}_.pth\")\n",
    "\n",
    "            elif(EARLY_STOP == True):\n",
    "                early_step += 1\n",
    "\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "\n",
    "\n",
    "        print(f\"Completed training fold {fold}, and best f1 score is {f1_score:.2f}\")\n",
    "        all_f1_scores.append(f1_score)\n",
    "\n",
    "    return f1_score, best_loss, all_f1_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model_and_predict(model, dataset, fold, model_path):\n",
    "    # Load the best model for a specific fold\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Create DataLoader for the test set\n",
    "    test_loader = DataLoader(dataset, batch_size = BATCH_SIZE)\n",
    "\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Convert the model output to probabilities using softmax\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim = 1)\n",
    "\n",
    "            # Get the predicted class for each sample\n",
    "            _, predictions = torch.max(probabilities, 1)\n",
    "\n",
    "            # Convert predictions to a NumPy array and append to the list\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_scores(all_fold_predictions, dataset):\n",
    "    f1_scores = []\n",
    "\n",
    "    for fold, predictions in enumerate(all_fold_predictions):\n",
    "        # Assuming binary classification; adjust the average parameter if needed\n",
    "        f1 = f1_score(dataset.labels, predictions, average = 'weighted')\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = openml.datasets.get_dataset(930)\n",
    "big_df, *_ = dataset.get_data()\n",
    "\n",
    "# Feature engineering\n",
    "processed_df = featureEnginering(big_df, train=True)\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "feature_names = list(filter(lambda x: x not in ['binaryClass'], processed_df.columns))\n",
    "\n",
    "X = processed_df[feature_names].values\n",
    "y = processed_df['binaryClass'].values\n",
    "\n",
    "# Create a Dataset object\n",
    "dataset = Dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# HyperParameters\n",
    "\n",
    "INPUT_SIZE = len(processed_df[feature_names].columns)\n",
    "OUTPUT_SIZE = len(processed_df['binaryClass'].unique())\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = 'cpu'\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = False\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "def pipeline(dataset, configs):\n",
    "    \n",
    "    # Perform grid search\n",
    "    best_configuration, best_score, scores, layers, activations = grid_search(configs, dataset)\n",
    "    \n",
    "    print('\\n')\n",
    "    print(f\"Best Configuration for the model: {best_configuration}\")\n",
    "    print(f\"Best F1-Score for the model: {round(100 * best_score, 2)}%\")\n",
    "\n",
    "\n",
    "    ## Creating Dataframe with results \n",
    "    grid_results_df = pd.DataFrame(columns = ['Layers', 'Activations'] + [f'Fold{i+1}' for i in range(5)])\n",
    "    grid_results_df['Layers'] = layers\n",
    "    grid_results_df['Activations'] = activations\n",
    "\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        vals = [lst[i] for lst in scores]\n",
    "        grid_results_df[f'Fold{i+1}'] = vals\n",
    "\n",
    "#     print(grid_results_df)\n",
    "\n",
    "\n",
    "    # Perform local search\n",
    "    best_configuration = {'layer_sizes': [64, 64], 'activations': [nn.ReLU(), nn.ReLU()]}\n",
    "    final_config, final_score = local_search(best_configuration, dataset)\n",
    "    \n",
    "    print(f\"Final Configuration for the model: {final_config}\")\n",
    "    print(f\"Final F1-Score for the model: {round(100 * final_score, 2)}%\")\n",
    "\n",
    "    # Train and evaluate the final model on the test set for all folds\n",
    "    all_fold_predictions = []\n",
    "\n",
    "    for fold in range(0, 5):  # Assuming 5 folds\n",
    "        model_path = f\"fold{fold}_.pth\"\n",
    "        print(model_path)\n",
    "        \n",
    "        model = SimpleNN(\n",
    "            input_size = INPUT_SIZE, output_size=OUTPUT_SIZE,\n",
    "            layer_sizes = final_config['layer_sizes'], activation_funcs = final_config['activations']\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        train_and_evaluate_model(model, dataset)\n",
    "\n",
    "        # Load the best model and make predictions\n",
    "        predictions = load_best_model_and_predict(model, dataset, fold, model_path)\n",
    "\n",
    "        # Append predictions to the list\n",
    "        all_fold_predictions.append(predictions)\n",
    "        \n",
    "        \n",
    "    # Calculate F1 scores for each fold\n",
    "    f1_scores = calculate_f1_scores(all_fold_predictions, dataset)\n",
    "\n",
    "    # Put F1 scores into a dataframe\n",
    "    f1_scores_df = pd.DataFrame({'Fold': [f'Fold{i+1}' for i in range(5)], 'F1 Score': f1_scores})\n",
    "\n",
    "    return grid_results_df, all_fold_predictions, f1_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer sizes: [64], Activations: ['ReLU']\n",
      "Completed training fold 0, and best f1 score is 0.85\n",
      "Completed training fold 1, and best f1 score is 0.82\n",
      "Completed training fold 2, and best f1 score is 0.85\n",
      "Completed training fold 3, and best f1 score is 0.87\n",
      "Completed training fold 4, and best f1 score is 0.84\n",
      "Layer sizes: [64, 64], Activations: ['ReLU', 'ReLU']\n",
      "Completed training fold 0, and best f1 score is 0.84\n",
      "Completed training fold 1, and best f1 score is 0.81\n",
      "Completed training fold 2, and best f1 score is 0.84\n",
      "Completed training fold 3, and best f1 score is 0.82\n",
      "Completed training fold 4, and best f1 score is 0.86\n",
      "Layer sizes: [128, 64], Activations: ['Tanh', 'ReLU']\n",
      "Completed training fold 0, and best f1 score is 0.81\n",
      "Completed training fold 1, and best f1 score is 0.78\n",
      "Completed training fold 2, and best f1 score is 0.85\n",
      "Completed training fold 3, and best f1 score is 0.83\n",
      "Completed training fold 4, and best f1 score is 0.85\n",
      "Layer sizes: [128, 64], Activations: ['Tanh', 'Tanh']\n",
      "Completed training fold 0, and best f1 score is 0.81\n",
      "Completed training fold 1, and best f1 score is 0.83\n",
      "Completed training fold 2, and best f1 score is 0.83\n",
      "Completed training fold 3, and best f1 score is 0.84\n",
      "Completed training fold 4, and best f1 score is 0.85\n",
      "Layer sizes: [64, 64, 32], Activations: ['ReLU', 'Tanh', 'ReLU']\n",
      "Completed training fold 0, and best f1 score is 0.74\n",
      "Completed training fold 1, and best f1 score is 0.84\n",
      "Completed training fold 2, and best f1 score is 0.82\n",
      "Completed training fold 3, and best f1 score is 0.84\n",
      "Completed training fold 4, and best f1 score is 0.86\n",
      "Layer sizes: [128, 128, 64], Activations: ['ReLU', 'ReLU', 'ReLU']\n",
      "Completed training fold 0, and best f1 score is 0.79\n",
      "Completed training fold 1, and best f1 score is 0.86\n",
      "Completed training fold 2, and best f1 score is 0.85\n",
      "Completed training fold 3, and best f1 score is 0.89\n",
      "Completed training fold 4, and best f1 score is 0.86\n",
      "Layer sizes: [128, 128, 64], Activations: ['Tanh', 'ReLU', 'ReLU']\n",
      "Completed training fold 0, and best f1 score is 0.83\n",
      "Completed training fold 1, and best f1 score is 0.85\n",
      "Completed training fold 2, and best f1 score is 0.85\n",
      "Completed training fold 3, and best f1 score is 0.84\n",
      "Completed training fold 4, and best f1 score is 0.80\n",
      "Best F1 Score: 0.80, Layer sizes: [128, 128, 64], Activations: ['Tanh', 'ReLU', 'ReLU']\n",
      "\n",
      "\n",
      "Best Configuration for the model: {'layer_sizes': [128, 128, 64], 'activations': [Tanh(), ReLU(), ReLU()]}\n",
      "Best F1-Score for the model: 80.05%\n",
      "Completed training fold 0, and best f1 score is 0.79\n",
      "Completed training fold 1, and best f1 score is 0.82\n",
      "Completed training fold 2, and best f1 score is 0.84\n",
      "Completed training fold 3, and best f1 score is 0.85\n",
      "Completed training fold 4, and best f1 score is 0.88\n",
      "Completed training fold 0, and best f1 score is 0.84\n",
      "Completed training fold 1, and best f1 score is 0.82\n",
      "Completed training fold 2, and best f1 score is 0.84\n",
      "Completed training fold 3, and best f1 score is 0.83\n",
      "Completed training fold 4, and best f1 score is 0.81\n",
      "Completed training fold 0, and best f1 score is 0.79\n",
      "Completed training fold 1, and best f1 score is 0.80\n",
      "Completed training fold 2, and best f1 score is 0.85\n",
      "Completed training fold 3, and best f1 score is 0.85\n",
      "Completed training fold 4, and best f1 score is 0.90\n",
      "Completed training fold 0, and best f1 score is 0.81\n",
      "Completed training fold 1, and best f1 score is 0.84\n",
      "Completed training fold 2, and best f1 score is 0.86\n",
      "Completed training fold 3, and best f1 score is 0.80\n",
      "Completed training fold 4, and best f1 score is 0.86\n",
      "Final Configuration for the model: {'layer_sizes': [64], 'activations': [ReLU()]}\n",
      "Final F1-Score for the model: 89.6%\n",
      "fold0_.pth\n",
      "Completed training fold 0, and best f1 score is 0.82\n",
      "Completed training fold 1, and best f1 score is 0.81\n",
      "Completed training fold 2, and best f1 score is 0.88\n",
      "Completed training fold 3, and best f1 score is 0.87\n",
      "Completed training fold 4, and best f1 score is 0.88\n",
      "fold1_.pth\n",
      "Completed training fold 0, and best f1 score is 0.82\n",
      "Completed training fold 1, and best f1 score is 0.87\n",
      "Completed training fold 2, and best f1 score is 0.90\n",
      "Completed training fold 3, and best f1 score is 0.85\n",
      "Completed training fold 4, and best f1 score is 0.92\n",
      "fold2_.pth\n",
      "Completed training fold 0, and best f1 score is 0.74\n",
      "Completed training fold 1, and best f1 score is 0.82\n",
      "Completed training fold 2, and best f1 score is 0.84\n",
      "Completed training fold 3, and best f1 score is 0.89\n",
      "Completed training fold 4, and best f1 score is 0.90\n",
      "fold3_.pth\n",
      "Completed training fold 0, and best f1 score is 0.85\n",
      "Completed training fold 1, and best f1 score is 0.81\n",
      "Completed training fold 2, and best f1 score is 0.89\n",
      "Completed training fold 3, and best f1 score is 0.83\n",
      "Completed training fold 4, and best f1 score is 0.88\n",
      "fold4_.pth\n",
      "Completed training fold 0, and best f1 score is 0.82\n",
      "Completed training fold 1, and best f1 score is 0.84\n",
      "Completed training fold 2, and best f1 score is 0.85\n",
      "Completed training fold 3, and best f1 score is 0.88\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File fold4_.pth cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m930\u001b[39m\n\u001b[0;32m      3\u001b[0m configs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m                 {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m64\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivations\u001b[39m\u001b[38;5;124m'\u001b[39m: [nn\u001b[38;5;241m.\u001b[39mReLU()]},\n\u001b[0;32m      5\u001b[0m                 {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivations\u001b[39m\u001b[38;5;124m'\u001b[39m: [nn\u001b[38;5;241m.\u001b[39mReLU(), nn\u001b[38;5;241m.\u001b[39mReLU()]},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m                 {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivations\u001b[39m\u001b[38;5;124m'\u001b[39m: [nn\u001b[38;5;241m.\u001b[39mReLU(), nn\u001b[38;5;241m.\u001b[39mReLU(), nn\u001b[38;5;241m.\u001b[39mReLU()]},\n\u001b[0;32m     10\u001b[0m                 {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivations\u001b[39m\u001b[38;5;124m'\u001b[39m: [nn\u001b[38;5;241m.\u001b[39mTanh(), nn\u001b[38;5;241m.\u001b[39mReLU(), nn\u001b[38;5;241m.\u001b[39mReLU()]}]\n\u001b[1;32m---> 12\u001b[0m grid_results_df, pipeline_results, f1_scores_df \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# print(pipeline_results)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m grid_results_df\n",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(dataset, configs)\u001b[0m\n\u001b[0;32m     39\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleNN(\n\u001b[0;32m     40\u001b[0m     input_size \u001b[38;5;241m=\u001b[39m INPUT_SIZE, output_size\u001b[38;5;241m=\u001b[39mOUTPUT_SIZE,\n\u001b[0;32m     41\u001b[0m     layer_sizes \u001b[38;5;241m=\u001b[39m final_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m], activation_funcs \u001b[38;5;241m=\u001b[39m final_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivations\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Load the best model and make predictions\u001b[39;00m\n\u001b[0;32m     48\u001b[0m predictions \u001b[38;5;241m=\u001b[39m load_best_model_and_predict(model, dataset, fold, model_path)\n",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[1;34m(model, dataset, n_splits)\u001b[0m\n\u001b[0;32m     39\u001b[0m     best_loss \u001b[38;5;241m=\u001b[39m valid_loss\n\u001b[0;32m     40\u001b[0m     f1_score \u001b[38;5;241m=\u001b[39m score\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfold\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m(EARLY_STOP \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     45\u001b[0m     early_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File fold4_.pth cannot be opened."
     ]
    }
   ],
   "source": [
    "dataset_id = 930\n",
    "\n",
    "configs = [\n",
    "                {'layer_sizes': [64], 'activations': [nn.ReLU()]},\n",
    "                {'layer_sizes': [64, 64], 'activations': [nn.ReLU(), nn.ReLU()]},\n",
    "                {'layer_sizes': [128, 64], 'activations': [nn.Tanh(), nn.ReLU()]},\n",
    "                {'layer_sizes': [128, 64], 'activations': [nn.Tanh(), nn.Tanh()]},\n",
    "                {'layer_sizes': [64, 64, 32], 'activations': [nn.ReLU(), nn.Tanh(), nn.ReLU()]},\n",
    "                {'layer_sizes': [128, 128, 64], 'activations': [nn.ReLU(), nn.ReLU(), nn.ReLU()]},\n",
    "                {'layer_sizes': [128, 128, 64], 'activations': [nn.Tanh(), nn.ReLU(), nn.ReLU()]}]\n",
    "\n",
    "grid_results_df, pipeline_results, f1_scores_df = pipeline(dataset, configs)\n",
    "# print(pipeline_results)\n",
    "grid_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best Parameters:  {'classifier__activation': 'logistic', 'classifier__alpha': 0.001, 'classifier__hidden_layer_sizes': (100, 50, 25)}\n",
      "Best F1 Score:  0.6752675359100018\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.623853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.661088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.695971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.679070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold  F1 Score\n",
       "0     1  0.652174\n",
       "1     2  0.623853\n",
       "2     3  0.661088\n",
       "3     4  0.695971\n",
       "4     5  0.679070"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=100)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'classifier__hidden_layer_sizes': [(50,50), (100,), (100,50,25)],\n",
    "    'classifier__activation': ['relu', 'tanh', 'logistic'],\n",
    "    'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Create a pipeline with scaling if needed\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), feature_names),  # Scale numeric features if needed\n",
    "        # You can add other transformers for categorical features if needed\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "    ('classifier', mlp)\n",
    "])\n",
    "\n",
    "# Create a StratifiedKFold cross-validator\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation and compute F1 score for each fold\n",
    "f1_scorer = make_scorer(f1_score, average='binary')  # 'binary' for binary classification\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring=f1_scorer, cv=cv, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the corresponding F1 score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best F1 Score: \", grid_search.best_score_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Compute F1 score for each fold using cross_val_score\n",
    "f1_scores = cross_val_score(best_model, X, y, cv=cv, scoring=f1_scorer)\n",
    "\n",
    "# Create a DataFrame to store F1 scores for each fold\n",
    "f1_df = pd.DataFrame({'Fold': range(1, len(f1_scores)+1), 'F1 Score': f1_scores})\n",
    "\n",
    "# Display the DataFrame\n",
    "f1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[LightGBM] [Info] Number of positive: 614, number of negative: 688\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4500\n",
      "[LightGBM] [Info] Number of data points in the train set: 1302, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.471582 -> initscore=-0.113794\n",
      "[LightGBM] [Info] Start training from score -0.113794\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best Parameters:  {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n",
      "Best F1 Score:  0.7367335409578714\n",
      "[LightGBM] [Info] Number of positive: 491, number of negative: 550\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4416\n",
      "[LightGBM] [Info] Number of data points in the train set: 1041, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.471662 -> initscore=-0.113474\n",
      "[LightGBM] [Info] Start training from score -0.113474\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 491, number of negative: 550\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4431\n",
      "[LightGBM] [Info] Number of data points in the train set: 1041, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.471662 -> initscore=-0.113474\n",
      "[LightGBM] [Info] Start training from score -0.113474\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 491, number of negative: 551\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000356 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4416\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.471209 -> initscore=-0.115291\n",
      "[LightGBM] [Info] Start training from score -0.115291\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 491, number of negative: 551\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4429\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.471209 -> initscore=-0.115291\n",
      "[LightGBM] [Info] Start training from score -0.115291\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 492, number of negative: 550\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4422\n",
      "[LightGBM] [Info] Number of data points in the train set: 1042, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.472169 -> initscore=-0.111440\n",
      "[LightGBM] [Info] Start training from score -0.111440\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.713178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.744186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.736462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.769841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold  F1 Score\n",
       "0     1  0.713178\n",
       "1     2  0.744186\n",
       "2     3  0.720000\n",
       "3     4  0.736462\n",
       "4     5  0.769841"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': [0.001, 0.01, 0.1],\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Create a pipeline with scaling if needed\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), feature_names),  # Scale numeric features if needed\n",
    "        # You can add other transformers for categorical features if needed\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "    ('classifier', lgbm)\n",
    "])\n",
    "\n",
    "# Create a StratifiedKFold cross-validator\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation and compute F1 score for each fold\n",
    "f1_scorer = make_scorer(f1_score, average='binary')  # 'binary' for binary classification\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring=f1_scorer, cv=cv, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and the corresponding F1 score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best F1 Score: \", grid_search.best_score_)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Compute F1 score for each fold using cross_val_score\n",
    "f1_scores = cross_val_score(best_model, X, y, cv=cv, scoring=f1_scorer)\n",
    "\n",
    "# Create a DataFrame to store F1 scores for each fold\n",
    "f1_df = pd.DataFrame({'Fold': range(1, len(f1_scores)+1), 'F1 Score': f1_scores})\n",
    "\n",
    "# Display the DataFrame\n",
    "f1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HZfdzixdjHKZ",
    "F3V4IImRxXIA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
