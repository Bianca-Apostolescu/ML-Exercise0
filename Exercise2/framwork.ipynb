{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HZfdzixdjHKZ",
        "F3V4IImRxXIA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelBinarizer,OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "hSUY6wbu1LA7"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pre (for Loan Dataset)"
      ],
      "metadata": {
        "id": "HZfdzixdjHKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def featureEnginering(df, train= False):\n",
        "  df = df.replace({\"10+ years\": 10, \"9 years\": 9,\"8 years\": 8,\"7 years\": 7,\n",
        "                                  \"6 years\": 6,\"5 years\": 5,\"4 years\": 4,\"3 years\": 3,\n",
        "                                  \"2 years\": 2,\"1 year\": 1, \"< 1 year\": 0})\n",
        "\n",
        "  df = df.drop(['verification_status', 'pymnt_plan', 'addr_state',\n",
        "           'initial_list_status', 'hardship_flag', 'disbursement_method', 'debt_settlement_flag',\n",
        "           # drop columns not issued to borrowers\n",
        "           'funded_amnt_inv', 'total_pymnt_inv', 'out_prncp_inv', 'funded_amnt_inv', 'funded_amnt'], axis=1)\n",
        "\n",
        "  # label_encoding\n",
        "  if train == True:\n",
        "    columnsTolabel = ['grade','application_type','term','loan_status']\n",
        "  else:\n",
        "    columnsTolabel = ['application_type' , 'term','loan_status']\n",
        "\n",
        "  for column in columnsTolabel:\n",
        "    label_encoder = LabelEncoder()\n",
        "    df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "  # onehot encoding\n",
        "  encoder = OneHotEncoder(sparse=False)\n",
        "  columnsToOnehot = ['purpose','home_ownership']\n",
        "  for column in columnsToOnehot:\n",
        "    encoded = encoder.fit_transform(df[[column]])\n",
        "    encoded = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())\n",
        "    df = pd.concat([df, encoded], axis=1)\n",
        "    df.drop(column, axis=1, inplace=True)\n",
        "\n",
        "  # Date-Time Features\n",
        "  df[\"credit_history_length\"] = (df['issue_d_year'] - df['earliest_cr_line_year']) * 12 + (df['issue_d_month'] - df['earliest_cr_line_month'])\n",
        "  df[\"time_since_last_payment\"] = -(df['issue_d_year'] - df['last_pymnt_d_year']) * 12 + (df['issue_d_month'] - df['last_pymnt_d_month'])\n",
        "  df[\"time_since_last_credit_pull\"] = -(df['issue_d_year'] - df['last_credit_pull_d_year']) * 12 + (df['issue_d_month'] - df['last_credit_pull_d_month'])\n",
        "  df[\"loan_age_at_last_payment\"] = (df['last_pymnt_d_year'] - df['issue_d_year']) * 12 + (df['last_pymnt_d_month'] - df['issue_d_month'])\n",
        "\n",
        "  return df\n",
        "\n",
        "train = pd.read_csv('/content/loan-10k.lrn.csv')\n",
        "train = featureEnginering(train,True)\n",
        "\n",
        "feature_names = list(\n",
        "    filter(lambda x: x not in ['grade', 'ID', 'kfold'], train.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N7N8XAGjE3C",
        "outputId": "d41b98e5-fc5a-4396-ab0f-788adecdb7b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "F3V4IImRxXIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.tensor(features, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "X = train[feature_names].values\n",
        "y = train['grade'].values\n",
        "dataset = Dataset(X, y)"
      ],
      "metadata": {
        "id": "zXHWjy94xWar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "auB7MQCDjJZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, layer_sizes, activation_funcs):\n",
        "        super(SimpleNN, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.batch_norms = nn.ModuleList()\n",
        "        self.dropouts = nn.ModuleList()\n",
        "        self.activation_funcs = activation_funcs\n",
        "\n",
        "        self.batch_norm1 = nn.BatchNorm1d(input_size)\n",
        "        # input layer\n",
        "        self.layers.append(nn.utils.weight_norm(nn.Linear(input_size, layer_sizes[0])))\n",
        "        self.batch_norms.append(nn.BatchNorm1d(layer_sizes[0]))\n",
        "        self.dropouts.append(nn.Dropout(0.2))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(layer_sizes)):\n",
        "            self.layers.append(nn.utils.weight_norm(nn.Linear(layer_sizes[i-1], layer_sizes[i])))\n",
        "            if i < len(layer_sizes) - 1:\n",
        "                self.batch_norms.append(nn.BatchNorm1d(layer_sizes[i]))\n",
        "            self.dropouts.append(nn.Dropout(0.5))\n",
        "\n",
        "        # Output layer\n",
        "        self.output = nn.utils.weight_norm(nn.Linear(layer_sizes[-1], output_size))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropouts[0](x)\n",
        "        x = self.layers[0](x)\n",
        "        x = self.activation_funcs[0](x)\n",
        "\n",
        "        for i in range(1, len(self.layers)):\n",
        "            if i < len(self.batch_norms):\n",
        "                x = self.batch_norms[i](x)\n",
        "            x = self.dropouts[i](x)\n",
        "            x = self.layers[i](x)\n",
        "            if i < len(self.activation_funcs):\n",
        "                x = self.activation_funcs[i](x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "XVRlXrkcIf0h"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mYPEAEJIjh7L"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search space"
      ],
      "metadata": {
        "id": "zA7kvkf_xeQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search(configs, dataset):\n",
        "\n",
        "    best_config = None\n",
        "\n",
        "    for config in configs:\n",
        "        best_score = 0\n",
        "        layer_size = config['layer_sizes']\n",
        "        activation = config['activations']\n",
        "\n",
        "        model = SimpleNN(\n",
        "            input_size = INPUT_SIZE, output_size = OUTPUT_SIZE,\n",
        "            layer_sizes = config['layer_sizes'],  activation_funcs = config['activations']\n",
        "        )\n",
        "\n",
        "        print(f\"Layer sizes: {layer_size}, Activations: {[type(act).__name__ for act in activation]}\")\n",
        "\n",
        "        score, loss = train_and_evaluate_model(model, dataset)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_config = config\n",
        "\n",
        "    print(f\"Best F1 Score: {best_score:.2f}, Layer sizes: {layer_size}, Activations: {[type(act).__name__ for act in activation]}\")\n",
        "    return best_config, best_score\n",
        "\n",
        "\n",
        "\n",
        "# Run the Grid Search\n",
        "configs = [\n",
        "    {'layer_sizes': [64, 64], 'activations': [nn.ReLU(), nn.ReLU()]},\n",
        "    {'layer_sizes': [128, 64], 'activations': [nn.Tanh(), nn.Tanh()]},\n",
        "    {'layer_sizes': [64, 64, 32], 'activations': [nn.ReLU(), nn.Tanh(), nn.ReLU()]}]\n",
        "\n",
        "best_configuration, best_score = grid_search(configs, dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zls7EJSvLAQG",
        "outputId": "29d11664-c44d-4532-d0ec-34487f8ca120"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer sizes: [64, 64], Activations: ['ReLU', 'ReLU']\n",
            "Completed training fold 0, and best f1 score is 0.82\n",
            "Completed training fold 1, and best f1 score is 0.81\n",
            "Completed training fold 2, and best f1 score is 0.77\n",
            "Completed training fold 3, and best f1 score is 0.70\n",
            "Completed training fold 4, and best f1 score is 0.69\n",
            "Layer sizes: [128, 64], Activations: ['Tanh', 'Tanh']\n",
            "Completed training fold 0, and best f1 score is 0.83\n",
            "Completed training fold 1, and best f1 score is 0.84\n",
            "Completed training fold 2, and best f1 score is 0.85\n",
            "Completed training fold 3, and best f1 score is 0.83\n",
            "Completed training fold 4, and best f1 score is 0.84\n",
            "Layer sizes: [64, 64, 32], Activations: ['ReLU', 'Tanh', 'ReLU']\n",
            "Completed training fold 0, and best f1 score is 0.81\n",
            "Completed training fold 1, and best f1 score is 0.83\n",
            "Completed training fold 2, and best f1 score is 0.85\n",
            "Completed training fold 3, and best f1 score is 0.84\n",
            "Completed training fold 4, and best f1 score is 0.84\n",
            "Best F1 Score: 0.83, Layer sizes: [64, 64, 32], Activations: ['ReLU', 'Tanh', 'ReLU']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def local_search(base_config, dataset, iterations=10):\n",
        "    def get_neighbors(configuration):\n",
        "      neighbors = []\n",
        "      # Example: Add a layer\n",
        "      neighbors.append({'layer_sizes': configuration['layer_sizes'] + [64],\n",
        "                        'activations': configuration['activations'] + [nn.ReLU()]})\n",
        "      # Example: Remove a layer\n",
        "      if len(configuration['layer_sizes']) > 1:\n",
        "          neighbors.append({'layer_sizes': configuration['layer_sizes'][:-1],\n",
        "                            'activations': configuration['activations'][:-1]})\n",
        "      return neighbors\n",
        "\n",
        "    def get_model(congfig):\n",
        "      layer_size = congfig['layer_sizes']\n",
        "      activation = congfig['activations']\n",
        "      model = SimpleNN( INPUT_SIZE, OUTPUT_SIZE, layer_size, activation)\n",
        "      return model\n",
        "\n",
        "    current_configuration = base_config\n",
        "    model = get_model(current_configuration)\n",
        "    current_score, current_loss = train_and_evaluate_model(model, dataset)\n",
        "\n",
        "    while True:\n",
        "        neighbors = get_neighbors(current_configuration)\n",
        "        any_improvement = False\n",
        "\n",
        "        for neighbor in neighbors:\n",
        "          neighbor = get_model(neighbor)\n",
        "          score , loss = train_and_evaluate_model(neighbor, dataset)\n",
        "\n",
        "          if score > current_score:\n",
        "              current_configuration = neighbor\n",
        "              current_score = score\n",
        "              any_improvement = True\n",
        "              break  # Move to the first improving neighbor\n",
        "\n",
        "        if not any_improvement:\n",
        "            break  # No improvement found, stop the search\n",
        "\n",
        "    return current_configuration, current_score\n",
        "\n",
        "\n",
        "# Run the Local Search\n",
        "base_config = {'layer_sizes': [64, 64], 'activations': [nn.ReLU(), nn.ReLU()]}\n",
        "best_configuration, best_score = local_search(base_config, dataset)\n"
      ],
      "metadata": {
        "id": "nhXZ5YdSkKqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "z7kmkUNjxi3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HyperParameters\n",
        "\n",
        "INPUT_SIZE=len(train[feature_names].columns)\n",
        "OUTPUT_SIZE=len(train['grade'].unique())\n",
        "\n",
        "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EARLY_STOPPING_STEPS = 10\n",
        "EARLY_STOP = False"
      ],
      "metadata": {
        "id": "I_d9TG1ixy6p"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(model, optimizer, scheduler, loss_fn, dataloader):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "\n",
        "    for data, target in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_fn(output, target.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        final_loss += loss.item()\n",
        "    final_loss /= len(dataloader)\n",
        "\n",
        "    return final_loss\n",
        "\n",
        "def valid_fn(model, loss_fn, dataloader):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    final_f1_score = 0\n",
        "\n",
        "    for data, target in dataloader:\n",
        "        outputs = model(data)\n",
        "        loss = loss_fn(outputs, target.long())\n",
        "        final_loss += loss.item()\n",
        "\n",
        "        preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
        "        final_f1_score += (f1_score(target.cpu().numpy(), preds.round(), average='weighted'))\n",
        "\n",
        "    final_f1_score /= len(dataloader)\n",
        "    final_loss /= len(dataloader)\n",
        "\n",
        "    return final_loss, final_f1_score"
      ],
      "metadata": {
        "id": "M3g7XKqfxuP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create and train a network\n",
        "def train_and_evaluate_model(model, dataset, n_splits=5):\n",
        "\n",
        "\n",
        "    labels = [label for _, label in dataset]\n",
        "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
        "\n",
        "    for fold, (train_ids, valid_ids) in enumerate(kfold.split(range(len(dataset)), labels)):\n",
        "\n",
        "        # Splitting the dataset\n",
        "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "        valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
        "\n",
        "        # Creating data loaders\n",
        "        train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_subsampler)\n",
        "        valid_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=valid_subsampler)\n",
        "\n",
        "        # initialization\n",
        "        model.to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n",
        "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(train_loader))\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        early_stopping_steps = EARLY_STOPPING_STEPS\n",
        "        early_step = 0\n",
        "\n",
        "\n",
        "        # Training loop\n",
        "        best_loss = np.inf\n",
        "        for epoch in range(EPOCHS):\n",
        "\n",
        "            train_loss = train_fn(model, optimizer,scheduler, loss_fn, train_loader)\n",
        "            valid_loss, score = valid_fn(model, loss_fn, valid_loader)\n",
        "\n",
        "            if valid_loss < best_loss:\n",
        "              best_loss = valid_loss\n",
        "              f1_score = score\n",
        "              torch.save(model.state_dict(), f\"fold{fold}_.pth\")\n",
        "\n",
        "            elif(EARLY_STOP == True):\n",
        "              early_step += 1\n",
        "              if (early_step >= early_stopping_steps):\n",
        "                  break\n",
        "\n",
        "\n",
        "        print(f\"Completed training fold {fold}, and best f1 score is {f1_score:.2f}\")\n",
        "\n",
        "    return best_score, best_loss"
      ],
      "metadata": {
        "id": "SSZXn1b4jRIO"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3M9fPbdBNoA"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a-O5aCkXCpkB"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}