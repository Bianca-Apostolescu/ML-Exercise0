\documentclass{article}
\usepackage{graphicx}
\usepackage{subcaption}

\begin{document}
\title{Machine Learning - Exercise 1: Classification}
\author{Bianca Apostolescu, Lu Chen, Matthias Glinzner}
\date{}
\maketitle

\section*{Dataset: Breast cancer}
\subsection*{Preprocessing}
For preprocessing the data set, two different scaling methods were used: Standardization and Normalization.
Because the data set contains no missing values and only one categorical value (the one we're testing for), no further data modification is necessary.

To detect outliers, four different methods were employed: Isolation Forest, Minimum Covariance Determinant, Local Outlier Factor and One-Class SVM.

The difference in quality these methods make will be discussed in a later section.

\subsection*{Classification}
\subsubsection*{K-Nearest Neighbours}
The first classification method we used is \textit{K-Nearest Neighbours}.
This method's performance is solid, especially after applying the right combination of preprocessing methods;
its cross-validation accuracy sits at $0.958$.

When compared to the default settings, this classifier's performance for the chosen dataset can be improved by switching the weight function to \textit{distance}.
Further, experimenting with the number of neighbours led to the value $17$ as optimal.

\subsubsection*{Random Forest}
The second method, \textit{Random Forest}, performs worst of the three with a cross-validation accuracy of only $0.947$.
However, it is most unaffected by switching scaling methods.
In fact, it performs almost optimally without any preprocessing at all.

We found this method to work best with default parameters.

\subsubsection*{Multilayer Perceptron}
The last method is \textit{Multilayer Perceptron}.
It outperforms the other two methods in all configurations; the cross-validation accuracy is $0.979$.

In terms of parameter-tweaking, setting the solver to \textit{lbfgs} yields the best results since the data set is relatively small.
Experimenting with different settings shows further that results can be improved even more by increasing the tolerance for optimization to $0.01$ and switching the activation function to \textit{tanh}.

\subsection*{Performance}
To measure performance we used accuracy scores and confusion matrices for the holdout method as well as accuracy for cross-validation.
When used without any preprocessing or parameter-tweaking, the \textit{Random Forest} method clearly outperforms the other two.
This picture changes once optimizations take place: MLP then yields the most promising results.

The tables show further that \textit{Random Forest}, unlike the other two methods, profits most from normalization, together with outlier detection via \textit{Minimum Covariance Determinant}.

\textit{K-NN}'s results are relatively homogenous within the same scaler class;
using standardization, \textit{Local Outlier Factor} performs slightly better.

Unfortunately \textit{MLP} doesn't converge fast enough when using normalization.
For standardization though, \textit{Isolation Forest} is the best choice.

Not surprisingly, each method yields slightly better results when validated via the holdout method;
the exception being \textit{K-NN}.

The confusion matrices show that save for \textit{K-NN} (normalized), false negatives are $0$.
And even the combination mentioned above has a recall value of $0.97$.
In terms of false positives it is also \textit{K-NN} that performs worst.

\begin{table}[h]
\begin{center}
\begin{tabular}{r|c|c}
& Accuracy score (holdout) & Accuracy score (cross-validation) \\
\hline
K-NN & 0.912 & 0.905 \\
Random Forest & 0.965 & 0.947 \\
MLP & 0.912 & 0.912 \\
\end{tabular}
\caption{Baseline performances (no preprocessing, default parameters).}
\end{center}
\end{table}

\begin{table}[h]
\begin{subtable}[h]{0.3\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
5 & 17 \\
\end{tabular}
\caption{K-NN}
\end{subtable}
\hfill
\begin{subtable}[h]{0.3\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
2 & 20 \\
\end{tabular}
\caption{Random Forest}
\end{subtable}
\hfill
\begin{subtable}[h]{0.3\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
5 & 17 \\
\end{tabular}
\caption{MLP}
\end{subtable}
\caption{Confusion matrices for baseline performance.}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{r|c|c}
& Accuracy score (holdout) & Accuracy score (cross-validation) \\
\hline
Isolation Forest	& 0.9473684211 & 0.95789 \\
Min. Covariance Det. & 0.9473684211	 & 0.95789 \\
Local Outlier Factor	& 0.9649122807 & 0.95789 \\
One-Class SVM & 0.9473684211	& 0.95789 \\
\end{tabular}
\caption{K-NN performance (Standardization, improved parameters).}
\end{center}
\end{table}

\begin{table}[h]
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
4 & 18 \\
\end{tabular}
\caption{Isolation Forest}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
3 & 19 \\
\end{tabular}
\caption{Min. Covariance Det.}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
2 & 20 \\
\end{tabular}
\caption{Local Outlier Factor}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
3 & 19 \\
\end{tabular}
\caption{One-Class SVM}
\end{subtable}
\caption{Confusion matrices K-NN performance (Standardization, improved parameters).}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{r|c|c}
& Accuracy score (holdout) & Accuracy score (cross-validation) \\
\hline
Isolation Forest	& 0.8947368421 & 0.90877 \\
Min. Covariance Det. & 0.8947368421 & 0.90877 \\
Local Outlier Factor	& 0.8947368421 & 0.90877 \\
One-Class SVM & 0.8947368421 & 0.90877 \\
\end{tabular}
\caption{K-NN performance (Normalization, improved parameters).}
\end{center}
\end{table}

\begin{table}[h]
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
34 & 1 \\
\hline
5 & 17 \\
\end{tabular}
\caption{Isolation Forest}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
34 & 1 \\
\hline
5 & 17 \\
\end{tabular}
\caption{Min. Covariance Det.}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
34 & 1 \\
\hline
5 & 17 \\
\end{tabular}
\caption{Local Outlier Factor}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
34 & 1 \\
\hline
5 & 17 \\
\end{tabular}
\caption{One-Class SVM}
\end{subtable}
\caption{Confusion matrices K-NN performance (Normalization, improved parameters).}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{r|c|c}
& Accuracy score (holdout) & Accuracy score (cross-validation) \\
\hline
Isolation Forest &	0.9649122807	&0.92982\\
Min. Covariance Det.	&0.9473684211&	0.93333\\
Local Outlier Factor&	0.9649122807	&0.9193\\
One-Class SVM&	0.9649122807&	0.92982\\
\end{tabular}
\caption{Random Forest performance (Standardization, improved parameters).}
\end{center}
\end{table}

\begin{table}[h]
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
2 & 20 \\
\end{tabular}
\caption{Isolation Forest}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
2 & 20 \\
\end{tabular}
\caption{Min. Covariance Det.}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
2 & 20 \\
\end{tabular}
\caption{Local Outlier Factor}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
2 & 20 \\
\end{tabular}
\caption{One-Class SVM}
\end{subtable}
\caption{Confusion matrices Random Forest performance (Standardization, improved parameters).}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{r|c|c}
& Accuracy score (holdout) & Accuracy score (cross-validation) \\
\hline
Isolation Forest	&0.9649122807	&0.94737\\
Min. Covariance Det.	&0.9824561404	&0.93684\\
Local Outlier Factor&	0.9649122807	&0.94035\\
One-Class SVM	&0.9473684211	&0.94386\\
\end{tabular}
\caption{Random Forest performance (Normalization, improved parameters).}
\end{center}
\end{table}

\begin{table}[h]
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
1 & 21 \\
\end{tabular}
\caption{Isolation Forest}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
1 & 21 \\
\end{tabular}
\caption{Min. Covariance Det.}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
3 & 19 \\
\end{tabular}
\caption{Local Outlier Factor}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
3 & 19 \\
\end{tabular}
\caption{One-Class SVM}
\end{subtable}
\caption{Confusion matrices Random Forest performance (Normalization, improved parameters).}
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{r|c|c}
& Accuracy score (holdout) & Accuracy score (cross-validation) \\
\hline
Isolation Forest	&1	&0.97895\\
Min. Covariance Det.	&0.9824561404	&0.97895\\
Local Outlier Factor	&0.9824561404	&0.97895\\
One-Class SVM	&0.9824561404	&0.97895\\
\end{tabular}
\caption{MLP performance (Standardization, improved parameters).}
\end{center}
\end{table}

\begin{table}[h]
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
0 & 22 \\
\end{tabular}
\caption{Isolation Forest}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
1 & 21 \\
\end{tabular}
\caption{Min. Covariance Det.}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
1 & 21 \\
\end{tabular}
\caption{Local Outlier Factor}
\end{subtable}
\hfill
\begin{subtable}[h]{0.2\textwidth}
\centering
\begin{tabular}{c|c}
35 & 0 \\
\hline
1 & 21 \\
\end{tabular}
\caption{One-Class SVM}
\end{subtable}
\caption{Confusion matrices MLP performance (Standardization, improved parameters)}
\end{table}

\end{document}